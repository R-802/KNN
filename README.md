# k-Nearest Neighbor

The k-Nearest Neighbors (KNN) algorithm is a straightforward classification or regression technique used in supervised learning. It operates on the principle that similar data points tend to share similar labels. Given a new data point, KNN identifies the \(k\) closest training data points (neighbors) and assigns the majority class label (for classification) or the average value (for regression) of these neighbors to the new point.

**Key points about k-Nearest Neighbor classification:**
- **Hyperparameter \(k\):** The algorithm's behavior hinges on the choice of the hyperparameter \(k\), which specifies the number of neighbors to consider when making predictions.
- **Distance metric:** KNN employs a distance metric, often the Euclidean distance, to quantify the similarity between data points. The Euclidean distance between two points $(x_{1}, y_{1})$ and $(x_{2}, y_{2})$ is given by:
  
$$\ \text{Euclidean Distance} = \sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2} \$$ 
  
 <p align="center"> In an $n$-dimensional space, the formula generalizes to: </center>
  
$$\ \text{Euclidean Distance} = \sqrt{\sum_{i=1}^{n} (x_{2} - x_{1})^2} \$$
  
 <p align="center"> Where $x_{1}$ and $x_{2}$ are the i-th components of the two points. </center>

- **Decision boundary:** KNN doesn't formulate a model; rather, it remembers the training data. Consequently, its decision boundary can be intricate and nonlinear, mirroring the distribution of the training data.
- **Scalability:** KNN's effectiveness may dwindle with large datasets, as it necessitates computing distances for each prediction.

## KNN Implementation with 3D Visualization

This repository contains a Python implementation showcasing the k-Nearest Neighbors (KNN) algorithm for 3-dimensional data and the visualization of its results using the `matplotlib` library.

### Implementation Steps

1. **Data Generation:** Random 3D data points are generated to simulate a dataset with three features. Binary labels (0 or 1) are assigned to each data point.

2. **KNN Class:** A `KNN` class is defined to encapsulate the KNN algorithm. It includes methods for fitting the model to training data and making predictions for new data points.

3. **Fitting:** The `fit` method of the `KNN` class stores the training data and corresponding labels.

4. **Prediction:** The `predict` method of the `KNN` class calculates predictions for new data points using the `_predict` method. The `_predict` method calculates distances to training data points, selects the \(k\) nearest neighbors, and predicts the class based on majority vote.

5. **Grid Generation:** Grid points are generated to visualize the decision boundary in 3D space. Ranges for each feature are defined, and `np.meshgrid` is used to create the grid.

6. **Visualization:** The `matplotlib` library's `Axes3D` module is employed to create a 3D scatter plot of the training data points. Points are color-coded based on their binary labels. The decision boundary is visualized using a contour plot, generated by predicting the class for each point in the grid.
